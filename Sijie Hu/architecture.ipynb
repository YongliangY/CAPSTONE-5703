{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始特征维度: 69, PCA后特征维度: 21\n",
      "训练样本数: 540, 验证样本数: 60, 测试样本数: 86, 类别数: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 设置随机种子确保可重复性\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. 加载训练集和测试集数据\n",
    "train_df = pd.read_csv(r'C:\\Users\\86135\\Desktop\\5703\\train_smote.csv')\n",
    "test_df = pd.read_csv(r'C:\\Users\\86135\\Desktop\\5703\\test_smote.csv')\n",
    "# 假设最后一列为标签，其他列为特征\n",
    "X_train_raw = train_df.iloc[:, :-1].values\n",
    "y_train = train_df.iloc[:, -1].values.astype(np.int64)\n",
    "X_test_raw = test_df.iloc[:, :-1].values\n",
    "y_test = test_df.iloc[:, -1].values.astype(np.int64)\n",
    "\n",
    "# 将标签从1-4转换为0-3（模型训练使用0索引的类别）\n",
    "y_train -= 1\n",
    "y_test -= 1\n",
    "\n",
    "# 2. 多视图特征预处理\n",
    "# 标准化（均值0，标准差1）\n",
    "scaler_std = StandardScaler()\n",
    "X_train_std = scaler_std.fit_transform(X_train_raw)\n",
    "X_test_std = scaler_std.transform(X_test_raw)\n",
    "\n",
    "# Min-Max归一化（0-1区间）\n",
    "scaler_mm = MinMaxScaler()\n",
    "X_train_mm = scaler_mm.fit_transform(X_train_raw)\n",
    "X_test_mm = scaler_mm.transform(X_test_raw)\n",
    "\n",
    "# PCA降维（保留95%方差信息量）\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "# 在PCA前先对特征标准化（PCA对尺度较敏感）\n",
    "X_train_for_pca = scaler_std.fit_transform(X_train_raw)  # 在原始训练特征上重新标准化\n",
    "X_train_pca = pca.fit_transform(X_train_for_pca)\n",
    "X_test_pca = pca.transform(scaler_std.transform(X_test_raw))\n",
    "print(f\"原始特征维度: {X_train_raw.shape[1]}, PCA后特征维度: {X_train_pca.shape[1]}\")\n",
    "\n",
    "# 3. 划分部分训练集作为验证集用于Early Stopping（例如10%）\n",
    "val_ratio = 0.1\n",
    "val_size = int(len(X_train_std) * val_ratio)\n",
    "if val_size > 0:\n",
    "    X_val_std = X_train_std[-val_size:];   X_train_std_ = X_train_std[:-val_size]\n",
    "    X_val_mm = X_train_mm[-val_size:];     X_train_mm_ = X_train_mm[:-val_size]\n",
    "    X_val_pca = X_train_pca[-val_size:];   X_train_pca_ = X_train_pca[:-val_size]\n",
    "    X_val_raw = X_train_raw[-val_size:];   X_train_raw_ = X_train_raw[:-val_size]\n",
    "    y_val = y_train[-val_size:];           y_train_ = y_train[:-val_size]\n",
    "else:\n",
    "    # 若训练集较小无法划分验证集，则不使用验证集早停\n",
    "    X_train_std_ = X_train_std; X_val_std = None\n",
    "    X_train_mm_ = X_train_mm;   X_val_mm = None\n",
    "    X_train_pca_ = X_train_pca; X_val_pca = None\n",
    "    X_train_raw_ = X_train_raw; X_val_raw = None\n",
    "    y_train_ = y_train;         y_val = None\n",
    "\n",
    "# 查看类别数和样本规模\n",
    "num_classes = len(np.unique(y_train_))\n",
    "print(f\"训练样本数: {len(y_train_)}, 验证样本数: {0 if y_val is None else len(y_val)}, 测试样本数: {len(y_test)}, 类别数: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Bayesian Neural Network子模块定义\n",
    "class BayesianLinear(nn.Module):\n",
    "    \"\"\"BNN的贝叶斯线性层，具有可学习的参数均值和rho用于计算标准差。\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # 权重参数的均值mu和rho（rho用于softplus计算sigma）\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.1))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5, -4))\n",
    "        # 偏置参数的均值mu和rho\n",
    "        self.bias_mu   = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.1))\n",
    "        self.bias_rho  = nn.Parameter(torch.Tensor(out_features).uniform_(-5, -4))\n",
    "    def forward(self, x, sample=True):\n",
    "        # 根据sample标志决定是否采样权重\n",
    "        if sample:\n",
    "            # 通过rho计算sigma，并采样权重和偏置噪声epsilon\n",
    "            weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
    "            bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
    "            W = self.weight_mu + weight_sigma * torch.randn_like(weight_sigma)\n",
    "            b = self.bias_mu + bias_sigma * torch.randn_like(bias_sigma)\n",
    "        else:\n",
    "            # 不采样，直接用均值\n",
    "            W = self.weight_mu\n",
    "            b = self.bias_mu\n",
    "        return F.linear(x, W, b)\n",
    "    def kl_loss(self):\n",
    "        # 计算该层权重和偏置的KL散度损失（与先验标准正态的KL）\n",
    "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
    "        kl = -0.5 * torch.sum(1 + 2*torch.log(weight_sigma) - self.weight_mu**2 - weight_sigma**2)\n",
    "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
    "        kl += -0.5 * torch.sum(1 + 2*torch.log(bias_sigma) - self.bias_mu**2 - bias_sigma**2)\n",
    "        return kl\n",
    "\n",
    "class BNN(nn.Module):\n",
    "    \"\"\"贝叶斯神经网络：若干BayesianLinear层 + ReLU + Dropout。\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[128, 64], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        # 隐藏层：BayesianLinear + ReLU + Dropout\n",
    "        for h_dim in hidden_dims:\n",
    "            self.layers.append(BayesianLinear(prev_dim, h_dim))\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = h_dim\n",
    "        # 输出层BayesianLinear（输出logits）\n",
    "        self.out_layer = BayesianLinear(prev_dim, output_dim)\n",
    "    def forward(self, x, sample=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BayesianLinear):\n",
    "                x = F.relu(layer(x, sample=sample))\n",
    "            else:\n",
    "                x = layer(x)  # Dropout层\n",
    "        x = self.out_layer(x, sample=sample)\n",
    "        return x\n",
    "    def total_kl_loss(self):\n",
    "        # 汇总所有BayesianLinear层的KL散度损失\n",
    "        kl_sum = 0.0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BayesianLinear):\n",
    "                kl_sum += layer.kl_loss()\n",
    "        kl_sum += self.out_layer.kl_loss()\n",
    "        return kl_sum\n",
    "\n",
    "# SimpleNet模型定义（简单全连接网络，用于替代原MAML子模型）\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"简单全连接网络，多层ReLU，全局输出。\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], num_classes=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ProtoNet模型定义（基于原型网络的Few-Shot分类模型）\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"ProtoNet的嵌入子网络，将输入映射到低维嵌入空间。\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "    \"\"\"原型网络：利用少样本学习思路进行分类。\"\"\"\n",
    "    def __init__(self, input_dim, embedding_dim=32, hidden_dim=128, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.embedding_net = EmbeddingNet(input_dim, hidden_dim, embedding_dim)\n",
    "        self.num_classes = num_classes\n",
    "        # 注册一个buffer用于存储每个类别的原型向量\n",
    "        self.register_buffer('prototypes', torch.zeros(num_classes, embedding_dim))\n",
    "    def set_prototypes(self, X_ref, y_ref):\n",
    "        \"\"\"\n",
    "        根据给定参考数据集计算每个类别的原型向量，并保存到self.prototypes。\n",
    "        X_ref: 张量, shape=(N, input_dim); y_ref: 张量, shape=(N,).\n",
    "        \"\"\"\n",
    "        self.eval()  # 切换到评估模式（如有Dropout则不启用）\n",
    "        with torch.no_grad():\n",
    "            embeds = self.embedding_net(X_ref)\n",
    "        prototypes = []\n",
    "        for c in range(self.num_classes):\n",
    "            class_embeds = embeds[y_ref == c]\n",
    "            if class_embeds.shape[0] == 0:\n",
    "                # 若该类别在参考集中无样本，则使用零向量占位\n",
    "                prototypes.append(torch.zeros(self.embedding_net.fc2.out_features))\n",
    "            else:\n",
    "                prototypes.append(class_embeds.mean(dim=0))\n",
    "        prototypes = torch.stack(prototypes)\n",
    "        self.prototypes.copy_(prototypes)  # 保存计算得到的原型\n",
    "    def forward(self, x):\n",
    "        # 计算输入样本与每个类原型的欧式距离，并输出负距离作为logits\n",
    "        emb = self.embedding_net(x)\n",
    "        diff = emb.unsqueeze(1) - self.prototypes.unsqueeze(0)  # shape: (batch, num_classes, embed_dim)\n",
    "        dist_sq = (diff * diff).sum(dim=2)                      # 计算平方距离\n",
    "        logits = -dist_sq                                       # 距离取负，距离越小logit越大\n",
    "        return logits\n",
    "\n",
    "# 基线DeepMLP模型定义\n",
    "class DeepMLP(nn.Module):\n",
    "    \"\"\"深度多层感知机，数层全连接+ReLU+Dropout。\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], num_classes=4, dropout_rate=0.4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# MetaLearner元学习器模型定义（用于融合子模型输出）\n",
    "class MetaLearner(nn.Module):\n",
    "    \"\"\"融合模型：接受所有子模型的预测作为输入，输出最终预测。\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32], output_dim=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 实例化各子模型，并分配对应的输入特征维度\n",
    "input_dim = X_train_raw_.shape[1]        # 原始特征维度 (例如69维)\n",
    "input_dim_pca = X_train_pca_.shape[1]    # PCA后特征维度 (例如21维)\n",
    "\n",
    "bnn_model = BNN(input_dim=input_dim, output_dim=num_classes, hidden_dims=[128, 64], dropout_rate=0.3)\n",
    "simple_model = SimpleNet(input_dim=input_dim, hidden_dims=[256, 128, 64], num_classes=num_classes)\n",
    "proto_model = ProtoNet(input_dim=input_dim_pca, hidden_dim=128, embedding_dim=32, num_classes=num_classes)\n",
    "baseline_model = DeepMLP(input_dim=input_dim, hidden_dims=[128, 64, 32], num_classes=num_classes, dropout_rate=0.4)\n",
    "\n",
    "# 为每个模型选择优化器\n",
    "bnn_optimizer = torch.optim.Adam(bnn_model.parameters(), lr=0.001)\n",
    "simple_optimizer = torch.optim.Adam(simple_model.parameters(), lr=0.001)\n",
    "proto_optimizer = torch.optim.Adam(proto_model.parameters(), lr=0.001)\n",
    "baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "# 定义损失函数（使用CrossEntropy，对于不均衡数据也可加入class_weight）\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 开始训练 BNN...\n",
      "Epoch 10/100, BNN Train Loss: 672.9616, Val Loss: 0.1518\n",
      "Epoch 20/100, BNN Train Loss: 643.2318, Val Loss: 0.0703\n",
      "Epoch 30/100, BNN Train Loss: 613.6834, Val Loss: 0.0439\n",
      "Epoch 40/100, BNN Train Loss: 592.3264, Val Loss: 0.0536\n",
      "BNN EarlyStopping at epoch 47, restoring best model.\n",
      "\n",
      ">> 开始训练 SimpleNet...\n",
      "Epoch 10/100, SimpleNet Train Loss: 0.2749, Val Loss: 0.2842\n",
      "Epoch 20/100, SimpleNet Train Loss: 0.1487, Val Loss: 0.1170\n",
      "Epoch 30/100, SimpleNet Train Loss: 0.1351, Val Loss: 0.0922\n",
      "Epoch 40/100, SimpleNet Train Loss: 0.1053, Val Loss: 0.0680\n",
      "Epoch 50/100, SimpleNet Train Loss: 0.0993, Val Loss: 0.0673\n",
      "SimpleNet EarlyStopping at epoch 59, restoring best model.\n",
      "\n",
      ">> 开始训练 ProtoNet...\n",
      "Episode 500/3000, ProtoNet Training Loss: 0.0176\n",
      "Episode 1000/3000, ProtoNet Training Loss: 0.0030\n",
      "Episode 1500/3000, ProtoNet Training Loss: 0.0002\n",
      "Episode 2000/3000, ProtoNet Training Loss: 0.0001\n",
      "Episode 2500/3000, ProtoNet Training Loss: 0.0100\n",
      "Episode 3000/3000, ProtoNet Training Loss: 0.0001\n",
      "\n",
      ">> 开始训练 Baseline MLP...\n",
      "Epoch 10/100, Baseline Train Loss: 0.2963, Val Loss: 0.1241\n",
      "Epoch 20/100, Baseline Train Loss: 0.1440, Val Loss: 0.0265\n",
      "Epoch 30/100, Baseline Train Loss: 0.1079, Val Loss: 0.0264\n",
      "Epoch 40/100, Baseline Train Loss: 0.0842, Val Loss: 0.0125\n",
      "Epoch 50/100, Baseline Train Loss: 0.0595, Val Loss: 0.0088\n",
      "Baseline MLP EarlyStopping at epoch 60, restoring best model.\n",
      "<< 子模型训练完成 >>\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 如GPU可用，可将模型移动到GPU加速训练\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_model.to(device)\n",
    "simple_model.to(device)\n",
    "proto_model.to(device)\n",
    "baseline_model.to(device)\n",
    "\n",
    "# 将训练和验证数据转换为Tensor并加载DataLoader\n",
    "X_train_std_t = torch.tensor(X_train_std_, dtype=torch.float32).to(device)\n",
    "X_train_mm_t = torch.tensor(X_train_mm_, dtype=torch.float32).to(device)\n",
    "X_train_pca_t = torch.tensor(X_train_pca_, dtype=torch.float32).to(device)\n",
    "X_train_raw_t = torch.tensor(X_train_raw_, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train_, dtype=torch.long).to(device)\n",
    "if y_val is not None:\n",
    "    X_val_std_t = torch.tensor(X_val_std, dtype=torch.float32).to(device)\n",
    "    X_val_mm_t = torch.tensor(X_val_mm, dtype=torch.float32).to(device)\n",
    "    X_val_pca_t = torch.tensor(X_val_pca, dtype=torch.float32).to(device)\n",
    "    X_val_raw_t = torch.tensor(X_val_raw, dtype=torch.float32).to(device)\n",
    "    y_val_t = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "else:\n",
    "    X_val_std_t = X_val_mm_t = X_val_pca_t = X_val_raw_t = y_val_t = None\n",
    "\n",
    "# 创建DataLoader用于分批训练\n",
    "batch_size = 32\n",
    "train_loader_std = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_std_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
    "train_loader_mm = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_mm_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
    "train_loader_pca = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_pca_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
    "train_loader_raw = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_raw_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 设置训练epoch数\n",
    "epochs_bnn = 100\n",
    "epochs_simple = 100\n",
    "epochs_base = 100\n",
    "episodes_proto = 3000  # ProtoNet按episode训练的迭代次数\n",
    "\n",
    "# 设置Early Stopping和学习率调度\n",
    "patience = 10  # 等待改善的epoch次数\n",
    "bnn_scheduler = ReduceLROnPlateau(bnn_optimizer, mode='min', factor=0.5, patience=3, verbose=False)\n",
    "simple_scheduler = ReduceLROnPlateau(simple_optimizer, mode='min', factor=0.5, patience=3, verbose=False)\n",
    "baseline_scheduler = ReduceLROnPlateau(baseline_optimizer, mode='min', factor=0.5, patience=3, verbose=False)\n",
    "# （ProtoNet的学习率调度在episodic循环中手动控制或根据需要添加）\n",
    "\n",
    "# 1. 训练 Bayesian Neural Network (BNN)\n",
    "print(\">> 开始训练 BNN...\")\n",
    "best_bnn_state = None\n",
    "best_val_loss = float('inf')\n",
    "no_improve = 0\n",
    "for epoch in range(1, epochs_bnn+1):\n",
    "    bnn_model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader_std:\n",
    "        # 前向传播（sample=True进行贝叶斯权重采样）\n",
    "        outputs = bnn_model(X_batch, sample=True)\n",
    "        ce_loss = criterion(outputs, y_batch)            # 交叉熵损失\n",
    "        kl_loss = bnn_model.total_kl_loss()              # KL散度损失\n",
    "        loss = ce_loss + 0.01 * kl_loss                  # 总损失 = CE + 0.01 * KL\n",
    "        # 反向传播与优化\n",
    "        bnn_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        bnn_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader_std)\n",
    "    # 验证集评估\n",
    "    if X_val_std_t is not None:\n",
    "        bnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = bnn_model(X_val_std_t, sample=False)  # 验证时采用均值权重\n",
    "            val_loss = criterion(val_outputs, y_val_t).item()\n",
    "        # 学习率调度\n",
    "        bnn_scheduler.step(val_loss)\n",
    "        # Early Stopping检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_bnn_state = {k: v.cpu() for k, v in bnn_model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"BNN EarlyStopping at epoch {epoch}, restoring best model.\")\n",
    "                if best_bnn_state:\n",
    "                    bnn_model.load_state_dict({k: v.to(device) for k, v in best_bnn_state.items()})\n",
    "                break\n",
    "    # 每10个epoch打印一次训练损失和当前验证损失\n",
    "    if epoch % 10 == 0:\n",
    "        val_log = f\", Val Loss: {val_loss:.4f}\" if X_val_std_t is not None else \"\"\n",
    "        print(f\"Epoch {epoch}/{epochs_bnn}, BNN Train Loss: {avg_loss:.4f}{val_log}\")\n",
    "# 如果训练完整未提前停止，也加载最佳状态\n",
    "if best_bnn_state:\n",
    "    bnn_model.load_state_dict({k: v.to(device) for k, v in best_bnn_state.items()})\n",
    "\n",
    "# 2. 训练 SimpleNet\n",
    "print(\"\\n>> 开始训练 SimpleNet...\")\n",
    "best_simple_state = None\n",
    "best_val_loss = float('inf')\n",
    "no_improve = 0\n",
    "for epoch in range(1, epochs_simple+1):\n",
    "    simple_model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader_mm:\n",
    "        outputs = simple_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        simple_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        simple_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader_mm)\n",
    "    # 验证\n",
    "    if X_val_mm_t is not None:\n",
    "        simple_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = simple_model(X_val_mm_t)\n",
    "            val_loss = criterion(val_outputs, y_val_t).item()\n",
    "        simple_scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_simple_state = {k: v.cpu() for k, v in simple_model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"SimpleNet EarlyStopping at epoch {epoch}, restoring best model.\")\n",
    "                if best_simple_state:\n",
    "                    simple_model.load_state_dict({k: v.to(device) for k, v in best_simple_state.items()})\n",
    "                break\n",
    "    if epoch % 10 == 0:\n",
    "        val_log = f\", Val Loss: {val_loss:.4f}\" if X_val_mm_t is not None else \"\"\n",
    "        print(f\"Epoch {epoch}/{epochs_simple}, SimpleNet Train Loss: {avg_loss:.4f}{val_log}\")\n",
    "if best_simple_state:\n",
    "    simple_model.load_state_dict({k: v.to(device) for k, v in best_simple_state.items()})\n",
    "\n",
    "# 3. 训练 ProtoNet（采用episodic训练方式）\n",
    "print(\"\\n>> 开始训练 ProtoNet...\")\n",
    "best_proto_state = None\n",
    "best_val_loss = float('inf')\n",
    "no_improve = 0\n",
    "proto_model.train()\n",
    "# 将训练数据转为tensor供ProtoNet使用\n",
    "X_train_pca_full_t = torch.tensor(X_train_pca_, dtype=torch.float32).to(device)\n",
    "y_train_full_t = torch.tensor(y_train_, dtype=torch.long).to(device)\n",
    "# 预先按类别组织索引，方便每次episode采样\n",
    "indices_by_class = {c: np.where(y_train_ == c)[0] for c in range(num_classes)}\n",
    "n_support = 5  # 每类支持集样本数\n",
    "n_query = 5    # 每类查询集样本数\n",
    "for episode in range(1, episodes_proto+1):\n",
    "    # 为每个类别随机采样support和query索引\n",
    "    support_indices = []\n",
    "    query_indices = []\n",
    "    for c in range(num_classes):\n",
    "        idx_all = indices_by_class[c]\n",
    "        if len(idx_all) < n_support + n_query:\n",
    "            idx = np.random.choice(idx_all, size=n_support+n_query, replace=True)\n",
    "        else:\n",
    "            idx = np.random.choice(idx_all, size=n_support+n_query, replace=False)\n",
    "        support_indices.extend(idx[:n_support])\n",
    "        query_indices.extend(idx[n_support:])\n",
    "    # 构造支持集和查询集张量\n",
    "    support_x = torch.tensor(X_train_pca_[support_indices], dtype=torch.float32).to(device)\n",
    "    support_y = torch.tensor(y_train_[support_indices], dtype=torch.long).to(device)\n",
    "    query_x = torch.tensor(X_train_pca_[query_indices], dtype=torch.float32).to(device)\n",
    "    query_y = torch.tensor(y_train_[query_indices], dtype=torch.long).to(device)\n",
    "    # 计算支持集样本的嵌入并更新临时原型\n",
    "    support_embeds = proto_model.embedding_net(support_x)\n",
    "    prototypes = []\n",
    "    for c in range(num_classes):\n",
    "        class_embeds = support_embeds[support_y == c]\n",
    "        if class_embeds.size(0) == 0:\n",
    "            prototypes.append(torch.zeros(proto_model.embedding_net.fc2.out_features).to(device))\n",
    "        else:\n",
    "            prototypes.append(class_embeds.mean(dim=0))\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    # 计算查询集样本到每个原型的距离并计算损失\n",
    "    query_embeds = proto_model.embedding_net(query_x)\n",
    "    dist_sq = ((query_embeds.unsqueeze(1) - prototypes.unsqueeze(0)) ** 2).sum(dim=2)\n",
    "    logits = -dist_sq\n",
    "    loss = criterion(logits, query_y)\n",
    "    proto_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    proto_optimizer.step()\n",
    "    # 每500个episode输出一次Loss\n",
    "    if episode % 500 == 0:\n",
    "        print(f\"Episode {episode}/{episodes_proto}, ProtoNet Training Loss: {loss.item():.4f}\")\n",
    "    # EarlyStopping: 每500个episode在验证集上评估一次\n",
    "    if (episode % 500 == 0) and (X_val_pca_t is not None):\n",
    "        proto_model.eval()\n",
    "        # 使用当前embedding_net计算训练集整体原型以评估验证集\n",
    "        proto_model.set_prototypes(X_train_pca_full_t, y_train_full_t)\n",
    "        with torch.no_grad():\n",
    "            val_logits = proto_model(X_val_pca_t)\n",
    "            val_loss = criterion(val_logits, y_val_t).item()\n",
    "        proto_model.train()\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_proto_state = {k: v.cpu() for k, v in proto_model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= 3:  # 等待3次评估无改善则停止\n",
    "                print(f\"ProtoNet EarlyStopping at episode {episode}, restoring best model.\")\n",
    "                if best_proto_state:\n",
    "                    proto_model.load_state_dict({k: v.to(device) for k, v in best_proto_state.items()})\n",
    "                break\n",
    "# 训练结束后，设置最终原型为全部训练集计算所得\n",
    "proto_model.eval()\n",
    "proto_model.set_prototypes(X_train_pca_full_t, y_train_full_t)\n",
    "proto_model.train()  # 切回train模式（虽然后续不再继续训练，但保持统一）\n",
    "\n",
    "# 如有保存的最佳状态，载入之\n",
    "if best_proto_state:\n",
    "    proto_model.load_state_dict({k: v.to(device) for k, v in best_proto_state.items()})\n",
    "\n",
    "# 4. 训练 Baseline DeepMLP\n",
    "print(\"\\n>> 开始训练 Baseline MLP...\")\n",
    "best_base_state = None\n",
    "best_val_loss = float('inf')\n",
    "no_improve = 0\n",
    "for epoch in range(1, epochs_base+1):\n",
    "    baseline_model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader_raw:\n",
    "        outputs = baseline_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        baseline_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        baseline_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader_raw)\n",
    "    # 验证\n",
    "    if X_val_raw_t is not None:\n",
    "        baseline_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = baseline_model(X_val_raw_t)\n",
    "            val_loss = criterion(val_outputs, y_val_t).item()\n",
    "        baseline_model.train()\n",
    "        baseline_scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_base_state = {k: v.cpu() for k, v in baseline_model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Baseline MLP EarlyStopping at epoch {epoch}, restoring best model.\")\n",
    "                if best_base_state:\n",
    "                    baseline_model.load_state_dict({k: v.to(device) for k, v in best_base_state.items()})\n",
    "                break\n",
    "    if epoch % 10 == 0:\n",
    "        val_log = f\", Val Loss: {val_loss:.4f}\" if X_val_raw_t is not None else \"\"\n",
    "        print(f\"Epoch {epoch}/{epochs_base}, Baseline Train Loss: {avg_loss:.4f}{val_log}\")\n",
    "if best_base_state:\n",
    "    baseline_model.load_state_dict({k: v.to(device) for k, v in best_base_state.items()})\n",
    "\n",
    "print(\"<< 子模型训练完成 >>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> 开始训练 MetaLearner...\n",
      "Epoch 10/100, MetaLearner Train Loss: 1.3223\n",
      "Epoch 20/100, MetaLearner Train Loss: 1.2166\n",
      "Epoch 30/100, MetaLearner Train Loss: 1.0906\n",
      "Epoch 40/100, MetaLearner Train Loss: 0.9064\n",
      "Epoch 50/100, MetaLearner Train Loss: 0.6857\n",
      "Epoch 60/100, MetaLearner Train Loss: 0.4690\n",
      "Epoch 70/100, MetaLearner Train Loss: 0.2942\n",
      "Epoch 80/100, MetaLearner Train Loss: 0.1784\n",
      "Epoch 90/100, MetaLearner Train Loss: 0.1045\n",
      "Epoch 100/100, MetaLearner Train Loss: 0.0836\n",
      "<< MetaLearner训练完成 >>\n",
      "\n",
      "Test set evaluation results：\n",
      " Accuracy: 0.8721\n",
      " Precision (Macro): 0.7740\n",
      " Recall (Macro): 0.7433\n",
      " F1-score (Macro): 0.7541\n",
      "Confusion Matrix (counts):\n",
      " [[34  1  1  1]\n",
      " [ 3  2  1  0]\n",
      " [ 2  1 35  0]\n",
      " [ 1  0  0  4]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# 将模型切换到评估模式\n",
    "bnn_model.eval(); simple_model.eval(); proto_model.eval(); baseline_model.eval()\n",
    "\n",
    "# 准备训练集的tensor数据（使用完整训练集，包括之前划分出的验证集部分，以充分利用数据训练MetaLearner）\n",
    "X_train_std_full_t = torch.tensor(X_train_std, dtype=torch.float32).to(device)\n",
    "X_train_mm_full_t = torch.tensor(X_train_mm, dtype=torch.float32).to(device)\n",
    "X_train_pca_full_t = torch.tensor(X_train_pca, dtype=torch.float32).to(device)\n",
    "X_train_raw_full_t = torch.tensor(X_train_raw, dtype=torch.float32).to(device)\n",
    "y_train_full = y_train  # 完整训练集标签 (numpy)\n",
    "\n",
    "# 获取每个子模型对完整训练集的预测概率\n",
    "with torch.no_grad():\n",
    "    bnn_probs_train = torch.softmax(bnn_model(X_train_std_full_t, sample=False), dim=1).cpu().numpy()\n",
    "    simple_probs_train = torch.softmax(simple_model(X_train_mm_full_t), dim=1).cpu().numpy()\n",
    "    proto_probs_train = torch.softmax(proto_model(X_train_pca_full_t), dim=1).cpu().numpy()\n",
    "    base_probs_train = torch.softmax(baseline_model(X_train_raw_full_t), dim=1).cpu().numpy()\n",
    "# 构造MetaLearner训练集特征 (N_train x 16)\n",
    "meta_train_X = np.concatenate([bnn_probs_train, simple_probs_train, proto_probs_train, base_probs_train], axis=1)\n",
    "meta_train_y = y_train_full\n",
    "\n",
    "# 初始化MetaLearner\n",
    "meta_input_dim = meta_train_X.shape[1]  # = num_classes * 4\n",
    "meta_model = MetaLearner(input_dim=meta_input_dim, hidden_dims=[64, 32], output_dim=num_classes).to(device)\n",
    "meta_optimizer = torch.optim.Adam(meta_model.parameters(), lr=0.001)\n",
    "\n",
    "# 计算类别权重（如需要处理训练集微小的不均衡）\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(meta_train_y), y=meta_train_y)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "meta_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 将MetaLearner训练数据转换为Tensor\n",
    "meta_train_X_t = torch.tensor(meta_train_X, dtype=torch.float32).to(device)\n",
    "meta_train_y_t = torch.tensor(meta_train_y, dtype=torch.long).to(device)\n",
    "\n",
    "# 训练MetaLearner（采用全数据每次一个epoch，因为meta_train_X规模相对较小）\n",
    "print(\"\\n>> 开始训练 MetaLearner...\")\n",
    "best_meta_state = None\n",
    "best_meta_loss = float('inf')\n",
    "no_improve = 0\n",
    "epochs_meta = 100\n",
    "for epoch in range(1, epochs_meta+1):\n",
    "    meta_model.train()\n",
    "    # 打乱顺序\n",
    "    perm = torch.randperm(meta_train_X_t.size(0))\n",
    "    X_perm = meta_train_X_t[perm]\n",
    "    y_perm = meta_train_y_t[perm]\n",
    "    # 前向与反向传播\n",
    "    outputs = meta_model(X_perm)\n",
    "    loss = meta_criterion(outputs, y_perm)\n",
    "    meta_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    meta_optimizer.step()\n",
    "    # 监控训练损失用于早停（MetaLearner无独立验证集，这里使用训练损失下降作为准则）\n",
    "    cur_loss = loss.item()\n",
    "    if cur_loss < best_meta_loss:\n",
    "        best_meta_loss = cur_loss\n",
    "        best_meta_state = {k: v.cpu() for k, v in meta_model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"MetaLearner EarlyStopping at epoch {epoch}, restoring best model.\")\n",
    "            if best_meta_state:\n",
    "                meta_model.load_state_dict({k: v.to(device) for k, v in best_meta_state.items()})\n",
    "            break\n",
    "    # 每10个epoch打印一次损失\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs_meta}, MetaLearner Train Loss: {cur_loss:.4f}\")\n",
    "# 加载最佳MetaLearner参数\n",
    "if best_meta_state:\n",
    "    meta_model.load_state_dict({k: v.to(device) for k, v in best_meta_state.items()})\n",
    "meta_model.eval()\n",
    "print(\"<< MetaLearner训练完成 >>\")\n",
    "\n",
    "# **在测试集上评估集成模型**\n",
    "# 将测试集数据转换为tensor并送入各子模型\n",
    "X_test_std_t = torch.tensor(X_test_std, dtype=torch.float32).to(device)\n",
    "X_test_mm_t = torch.tensor(X_test_mm, dtype=torch.float32).to(device)\n",
    "X_test_pca_t = torch.tensor(X_test_pca, dtype=torch.float32).to(device)\n",
    "X_test_raw_t = torch.tensor(X_test_raw, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    bnn_probs_test = torch.softmax(bnn_model(X_test_std_t, sample=False), dim=1).cpu().numpy()\n",
    "    simple_probs_test = torch.softmax(simple_model(X_test_mm_t), dim=1).cpu().numpy()\n",
    "    proto_probs_test = torch.softmax(proto_model(X_test_pca_t), dim=1).cpu().numpy()\n",
    "    base_probs_test = torch.softmax(baseline_model(X_test_raw_t), dim=1).cpu().numpy()\n",
    "# 构造MetaLearner测试集输入特征\n",
    "meta_test_X = np.concatenate([bnn_probs_test, simple_probs_test, proto_probs_test, base_probs_test], axis=1)\n",
    "meta_test_X_t = torch.tensor(meta_test_X, dtype=torch.float32).to(device)\n",
    "# MetaLearner预测输出\n",
    "with torch.no_grad():\n",
    "    meta_logits = meta_model(meta_test_X_t)\n",
    "    meta_pred = torch.argmax(meta_logits, dim=1).cpu().numpy()\n",
    "y_true = y_test  # 测试集真实标签 (numpy数组)\n",
    "y_pred = meta_pred\n",
    "\n",
    "# 计算评估指标 (Accuracy, Precision, Recall, F1-score)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "print(\"\\nTest set evaluation results：\")\n",
    "print(f\" Accuracy: {accuracy:.4f}\")\n",
    "print(f\" Precision (Macro): {precision:.4f}\")\n",
    "print(f\" Recall (Macro): {recall:.4f}\")\n",
    "print(f\" F1-score (Macro): {f1:.4f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2,3])\n",
    "print(\"Confusion Matrix (counts):\\n\", cm)\n",
    "\n",
    "# 绘制混淆矩阵热力图\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[1,2,3,4], yticklabels=[1,2,3,4])\n",
    "plt.title(\"Confusion Matrix - Optimized Ensemble\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
