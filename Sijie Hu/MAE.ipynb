{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8a9733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the Excel file:\n",
      "['Paper No', 'Specimen', 'conformity_tbec2018', 'tw', 'lw', 'hw', 'M/(V.lw)', 'hw/lw', 'P/(Ag.fc)', 'fc', 'Agb', 'Ag', 'Agb/Ag', 'ρbl.fybl', 'ρsh.fysh', 'ρl.fyl', 'ρt.fyt', 'v_test', 'failure_mode']\n",
      "Data Preview:\n",
      "                      Paper No                   Specimen  \\\n",
      "0  Abdulridha & Palermo (2017)                      W1-SR   \n",
      "1      [114] Adajar et al.1995                       RCW1   \n",
      "2                          NaN                       RCW3   \n",
      "3             [98] Adebar,2007  High-Rise Core Wall (265)   \n",
      "4            [26] Alarcon,2014                    W1 (49)   \n",
      "\n",
      "   conformity_tbec2018     tw    lw       hw  M/(V.lw)     hw/lw  P/(Ag.fc)  \\\n",
      "0                  0.0  150.0  1000   2200.0      2.20  2.200000       0.00   \n",
      "1                  0.0  150.0  1400   2000.0      1.43  1.428571       0.01   \n",
      "2                  0.0  150.0  1400   2000.0      1.43  1.428571       0.01   \n",
      "3                  0.0  127.0  1625  12000.0      7.38  7.384615       0.10   \n",
      "4                  0.0  100.0   700   1600.0      2.50  2.285714       0.15   \n",
      "\n",
      "     fc      Agb        Ag    Agb/Ag   ρbl.fybl  ρsh.fysh    ρl.fyl    ρt.fyt  \\\n",
      "0  30.5  30000.0  150000.0  0.200000   5.652500  7.395000  2.847500  3.740000   \n",
      "1  46.8      0.0  210000.0  0.000000   0.000000  0.000000  4.410368  2.021229   \n",
      "2  46.6      0.0  210000.0  0.000000   0.000000  0.000000  7.725619  2.021229   \n",
      "3  49.0  77140.0  309093.0  0.249569   2.956163  2.667677  1.219476  1.177428   \n",
      "4  27.4  10000.0   70000.0  0.142857  14.732880  0.000000  2.673600  2.655481   \n",
      "\n",
      "   v_test  failure_mode  \n",
      "0  155.15           3.0  \n",
      "1  666.82           3.0  \n",
      "2  787.40           1.0  \n",
      "3  144.75           3.0  \n",
      "4  143.15           2.0  \n",
      "Training model 1...\n",
      "Model 1, Epoch 30/150, Loss: nan\n",
      "Model 1, Epoch 60/150, Loss: nan\n",
      "Model 1, Epoch 90/150, Loss: nan\n",
      "Model 1, Epoch 120/150, Loss: nan\n",
      "Model 1, Epoch 150/150, Loss: nan\n",
      "Training model 2...\n",
      "Model 2, Epoch 30/150, Loss: nan\n",
      "Model 2, Epoch 60/150, Loss: nan\n",
      "Model 2, Epoch 90/150, Loss: nan\n",
      "Model 2, Epoch 120/150, Loss: nan\n",
      "Model 2, Epoch 150/150, Loss: nan\n",
      "Training model 3...\n",
      "Model 3, Epoch 30/150, Loss: nan\n",
      "Model 3, Epoch 60/150, Loss: nan\n",
      "Model 3, Epoch 90/150, Loss: nan\n",
      "Model 3, Epoch 120/150, Loss: nan\n",
      "Model 3, Epoch 150/150, Loss: nan\n",
      "Training model 4...\n",
      "Model 4, Epoch 30/150, Loss: nan\n",
      "Model 4, Epoch 60/150, Loss: nan\n",
      "Model 4, Epoch 90/150, Loss: nan\n",
      "Model 4, Epoch 120/150, Loss: nan\n",
      "Model 4, Epoch 150/150, Loss: nan\n",
      "Training model 5...\n",
      "Model 5, Epoch 30/150, Loss: nan\n",
      "Model 5, Epoch 60/150, Loss: nan\n",
      "Model 5, Epoch 90/150, Loss: nan\n",
      "Model 5, Epoch 120/150, Loss: nan\n",
      "Model 5, Epoch 150/150, Loss: nan\n",
      "Ensemble Model Accuracy: 0.42\n",
      "Confusion Matrix:\n",
      "[[42  0  0]\n",
      " [29  0  0]\n",
      " [29  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      1.00      0.59        42\n",
      "           1       0.00      0.00      0.00        29\n",
      "           2       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.42       100\n",
      "   macro avg       0.14      0.33      0.20       100\n",
      "weighted avg       0.18      0.42      0.25       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15842\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\15842\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\15842\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file (ensure the correct file path)\n",
    "data = pd.read_excel(\"database.xlsx\")\n",
    "\n",
    "# Print all column names to verify actual column names\n",
    "print(\"Column names in the Excel file:\")\n",
    "print(data.columns.tolist())\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Data Loading and Preprocessing\n",
    "# -----------------------------\n",
    "data = pd.read_excel(\"database.xlsx\")\n",
    "print(\"Data Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Define feature columns and target column (modify as needed)\n",
    "feature_cols = ['tw', 'lw', 'hw', 'M/(V.lw)', 'hw/lw', 'P/(Ag.fc)', 'fc', 'Agb', 'Ag', 'Agb/Ag', 'ρbl.fybl', 'ρsh.fysh', 'ρl.fyl', 'ρt.fyt', 'v_test']\n",
    "target_col = \"failure_mode\"\n",
    "\n",
    "X = data[feature_cols].values\n",
    "y = data[target_col].values\n",
    "\n",
    "# Normalize features to the range [-1,1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the target variable\n",
    "y_encoded = pd.get_dummies(y).values\n",
    "\n",
    "# Split data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Convert one-hot encoded targets to class labels (integers) for CrossEntropyLoss\n",
    "y_train_labels = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long).to(device)\n",
    "y_test_labels = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Custom Dataset and DataLoader\n",
    "# -----------------------------\n",
    "class FailureModeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y  # Target variable as class labels\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = FailureModeDataset(X_train_tensor, y_train_labels)\n",
    "test_dataset = FailureModeDataset(X_test_tensor, y_test_labels)\n",
    "\n",
    "# Compute class weights based on training set distribution\n",
    "unique_classes, counts = np.unique(y_train_labels.cpu().numpy(), return_counts=True)\n",
    "class_weights = 1. / counts\n",
    "weights = [class_weights[label] for label in y_train_labels.cpu().numpy()]\n",
    "weights = torch.DoubleTensor(weights)\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define 5-Layer Neural Network (Improved Version)\n",
    "# -----------------------------\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DeepNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 40)\n",
    "        self.fc2 = nn.Linear(40, 80)\n",
    "        self.fc3 = nn.Linear(80, 60)\n",
    "        self.fc4 = nn.Linear(60, 40)\n",
    "        self.fc5 = nn.Linear(40, 25)\n",
    "        self.output = nn.Linear(25, output_dim)\n",
    "        self.activation = nn.ReLU()  # Using ReLU activation function\n",
    "        self.dropout = nn.Dropout(0.2)  # Increase Dropout rate\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.activation(self.fc5(x))\n",
    "        x = self.output(x)\n",
    "        return x  # Return raw logits, CrossEntropyLoss will handle softmax\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train Multiple Models (MAE)\n",
    "# -----------------------------\n",
    "ensemble_size = 5\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(unique_classes)  # Number of output classes\n",
    "models = []\n",
    "\n",
    "# Compute class weights tensor for loss function\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "for i in range(ensemble_size):\n",
    "    print(f\"Training model {i+1}...\")\n",
    "    model = DeepNeuralNetwork(input_dim, output_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    # Use weighted cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    # Learning rate scheduler: Reduce LR every 30 epochs\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    epochs = 150\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "        scheduler.step()\n",
    "        # Print average loss every 30 epochs\n",
    "        if (epoch+1) % 30 == 0:\n",
    "            avg_loss = running_loss / len(train_dataset)\n",
    "            print(f\"Model {i+1}, Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    models.append(model)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Ensemble Prediction & Model Evaluation\n",
    "# -----------------------------\n",
    "def ensemble_predict(models, loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for X_batch, y_batch in loader:\n",
    "        batch_preds = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_batch)\n",
    "                # Apply softmax to get class probabilities\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                batch_preds.append(probs.cpu().numpy())\n",
    "        avg_preds = np.mean(batch_preds, axis=0)\n",
    "        all_preds.append(avg_preds)\n",
    "        all_labels.append(y_batch.cpu().numpy())\n",
    "    return np.concatenate(all_preds), np.concatenate(all_labels)\n",
    "\n",
    "ensemble_output, y_true = ensemble_predict(models, test_loader)\n",
    "y_pred = np.argmax(ensemble_output, axis=1)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Ensemble Model Accuracy:\", acc)\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a4f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
