{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNz3TUJ4hdT7nZOqJRh16fV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lBo_zCNvLG2D"},"outputs":[],"source":["#initial DE component\n","import numpy as np\n","import tensorflow as tf\n","\n","# Load trained deep learning models\n","base_m1 = tf.keras.models.load_model(\"model_1.h5\")\n","base_m2 = tf.keras.models.load_model(\"model_2.h5\")\n","base_m3 = tf.keras.models.load_model(\"model_3.h5\")\n","\n","# Load validation data\n","X_val = np.load(\"X_val.npy\")\n","y_val = np.load(\"y_val.npy\")\n","\n","# Get model predictions on validation data\n","preds_1 = base_m1.predict(X_val)\n","preds_2 = base_m2.predict(X_val)\n","preds_3 = base_m3.predict(X_val)\n","\n","# Stack predictions (num_models, num_samples, num_classes)\n","predictions = np.array([preds_1, preds_2, preds_3])\n","num_models, num_samples, num_classes = predictions.shape\n","\n","# Flatten predictions for easier computation\n","predictions = predictions.reshape(num_models, -1)\n","\n","# DE hyperparameters\n","POP_SIZE = 20   # Number of candidate weight vectors\n","F = 0.6         # Scaling factor for mutation (typically 0.4 - 1.2)\n","CR = 0.9        # Crossover probability\n","MAX_ITER = 100  # Maximum iterations\n","\n","# Initialize a population of weight vectors (each weight vector sums to 1)\n","population = np.random.dirichlet(np.ones(num_models), size=POP_SIZE)\n"]},{"cell_type":"code","source":["#mutation function\n","def mutate(population, F):\n","    \"\"\"\n","    Perform mutation in Differential Evolution (DE).\n","\n","    - population: (POP_SIZE, num_models) matrix of weight vectors.\n","    - F: Scaling factor for mutation.\n","\n","    Returns:\n","    - mutant_vectors: New mutated weight vectors.\n","    \"\"\"\n","    pop_size, dimensions = population.shape\n","    mutant_vectors = np.zeros((pop_size, dimensions))\n","\n","    for i in range(pop_size):\n","        # Select 3 random distinct indices r1, r2, r3 different from i\n","        indices = np.random.choice([j for j in range(pop_size) if j != i], 3, replace=False)\n","        r1, r2, r3 = indices\n","\n","        # Compute mutant vector using DE formula: Vi = Xr1 + F * (Xr2 - Xr3)\n","        mutant_vector = population[r1] + F * (population[r2] - population[r3])\n","\n","        # Ensure weights are between 0 and 1, and normalize to sum to 1\n","        mutant_vector = np.clip(mutant_vector, 0, 1)\n","        mutant_vector /= np.sum(mutant_vector)\n","\n","        mutant_vectors[i] = mutant_vector\n","\n","    return mutant_vectors\n"],"metadata":{"id":"4RTD4IPZjHta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#crossover function\n","def crossover(mutants, population, CR):\n","    \"\"\"\n","    Perform crossover in DE to create trial vectors.\n","\n","    - mutants: Mutated population.\n","    - population: Original population.\n","    - CR: Crossover probability.\n","\n","    Returns:\n","    - trial_vectors: Offspring vectors after crossover.\n","    \"\"\"\n","    pop_size, dimensions = population.shape\n","    trial_vectors = np.zeros((pop_size, dimensions))\n","\n","    for i in range(pop_size):\n","        # Perform binomial crossover\n","        crossover_mask = np.random.rand(dimensions) < CR  # Boolean mask\n","        trial_vector = np.where(crossover_mask, mutants[i], population[i])\n","\n","        # Ensure the new weight vector remains valid\n","        trial_vector = np.clip(trial_vector, 0, 1)\n","        trial_vector /= np.sum(trial_vector)\n","\n","        trial_vectors[i] = trial_vector\n","\n","    return trial_vectors\n"],"metadata":{"id":"ULklzCPz5MGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define loss\n","def evaluate(weights):\n","    \"\"\"\n","    Compute the loss (categorical cross-entropy) for a given weight vector.\n","\n","    - weights: Weight vector for combining models.\n","\n","    Returns:\n","    - Loss value (lower is better).\n","    \"\"\"\n","    weighted_preds = np.dot(weights, predictions).reshape(num_samples, num_classes)  # Weighted sum\n","    loss = tf.keras.losses.categorical_crossentropy(y_val, weighted_preds).numpy()\n","    return np.mean(loss)  # Return mean loss\n"],"metadata":{"id":"1zJf8w5I5VQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#optimization\n","best_weights = None\n","best_loss = float(\"inf\")\n","\n","for iteration in range(MAX_ITER):\n","    # Step 1: Mutation\n","    mutants = mutate(population, F)\n","\n","    # Step 2: Crossover\n","    trial_vectors = crossover(mutants, population, CR)\n","\n","    # Step 3: Selection\n","    for i in range(POP_SIZE):\n","        trial_loss = evaluate(trial_vectors[i])\n","        original_loss = evaluate(population[i])\n","\n","        if trial_loss < original_loss:  # Select the better weight vector\n","            population[i] = trial_vectors[i]\n","\n","            # Track the best solution\n","            if trial_loss < best_loss:\n","                best_loss = trial_loss\n","                best_weights = trial_vectors[i]\n","\n","    # Print progress\n","    print(f\"Iteration {iteration + 1}/{MAX_ITER} - Best Loss: {best_loss:.5f}\")\n","\n","# Final optimized weights\n","print(\"Optimized Weights:\", best_weights)\n"],"metadata":{"id":"S8v1Wzc_8H_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute ensemble predictions with optimized weights\n","final_predictions = np.dot(best_weights, predictions).reshape(num_samples, num_classes)\n","\n","# Convert predictions to class labels\n","final_labels = np.argmax(final_predictions, axis=1)\n","true_labels = np.argmax(y_val, axis=1)\n","\n","# Compute accuracy\n","accuracy = np.mean(final_labels == true_labels)\n","print(\"Final Ensemble Accuracy:\", accuracy)\n"],"metadata":{"id":"ahvE7v7Q8NrE"},"execution_count":null,"outputs":[]}]}