{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T06:00:43.090627Z",
     "start_time": "2025-04-20T06:00:40.758977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import skopt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import dimensions_aslist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "dimensions_aslist.original = dimensions_aslist\n",
    "def patched_dimensions_aslist(search_space):\n",
    "    return [d if isinstance(d, Categorical) else d for d in search_space]\n",
    "skopt.utils.dimensions_aslist = patched_dimensions_aslist\n",
    "# Fix random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ],
   "id": "f15a7a8e5fdde1c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T06:00:43.122943Z",
     "start_time": "2025-04-20T06:00:43.101641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Enhanced Data Loading and Preprocessing\n",
    "def load_data():\n",
    "    train_df = pd.read_csv('train_smote.csv')\n",
    "    test_df = pd.read_csv('test_smote.csv')\n",
    "\n",
    "    # Feature scaling (normalization)\n",
    "    train_features = train_df.drop(columns=['failure mode']).values.astype(np.float32)\n",
    "    test_features = test_df.drop(columns=['failure mode']).values.astype(np.float32)\n",
    "\n",
    "    # Normalize using training set statistics\n",
    "    feature_mean = train_features.mean(axis=0)\n",
    "    feature_std = train_features.std(axis=0) + 1e-8  # Avoid division by zero\n",
    "    train_features = (train_features - feature_mean) / feature_std\n",
    "    test_features = (test_features - feature_mean) / feature_std\n",
    "\n",
    "    # Labels (assuming classes start from 1)\n",
    "    train_labels = (train_df['failure mode'].values - 1).astype(np.int64)\n",
    "    test_labels = (test_df['failure mode'].values - 1).astype(np.int64)\n",
    "\n",
    "    # ===== 新增数据验证 =====\n",
    "    print(\"\\n=== Data shape verification ===\")\n",
    "    print(f\"Training data shape: {train_features.shape}\")\n",
    "    print(f\"Training label shape: {train_labels.shape}\")\n",
    "    print(f\"Test data shape: {test_features.shape}\")\n",
    "    print(f\"Test label shape: {test_labels.shape}\")\n",
    "    print(f\"Unique label value: {np.unique(train_labels)}\")\n",
    "\n",
    "    assert train_features.shape[1] == test_features.shape[1], \"The training and testing feature dimensions are inconsistent!\"\n",
    "    assert len(np.unique(train_labels)) == len(np.unique(test_labels)), \"The training and test label categories are inconsistent!\"\n",
    "\n",
    "\n",
    "    return (train_features, train_labels), (test_features, test_labels)\n",
    "\n",
    "(train_features, train_labels), (test_features, test_labels) = load_data()\n",
    "\n",
    "# 2. Improved Bayesian Neural Network Implementation\n",
    "\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, prior_sigma1=1.0, prior_sigma2=0.1, prior_pi=0.5):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Weight parameters (mean and rho)\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.1))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5, -4))\n",
    "\n",
    "        # Bias parameters (mean and rho)\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-5, -4))\n",
    "\n",
    "        # Scale mixture prior parameters\n",
    "        self.prior_sigma1 = prior_sigma1\n",
    "        self.prior_sigma2 = prior_sigma2\n",
    "        self.prior_pi = prior_pi\n",
    "\n",
    "        # Initialize prior distributions\n",
    "        self.weight_prior = self.scale_mixture_prior()\n",
    "        self.bias_prior = self.scale_mixture_prior()\n",
    "\n",
    "        # For KL divergence approximation\n",
    "        self.kl = 0\n",
    "\n",
    "    def scale_mixture_prior(self):\n",
    "        mix = torch.distributions.Categorical(torch.tensor([self.prior_pi, 1.0 - self.prior_pi]))\n",
    "        comp = torch.distributions.Normal(\n",
    "            torch.tensor([0.0, 0.0]),\n",
    "            torch.tensor([self.prior_sigma1, self.prior_sigma2])\n",
    "        )\n",
    "        return torch.distributions.MixtureSameFamily(mix, comp)\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "\n",
    "        assert x.size(-1) == self.in_features, \\\n",
    "        f\"输入特征维度{x.size(-1)}与层输入特征数{self.in_features}不匹配\"\n",
    "\n",
    "        if sample:\n",
    "            weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
    "            bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
    "\n",
    "            assert self.weight_mu.shape == (self.out_features, self.in_features), \\\n",
    "            f\"权重mu形状应为{(self.out_features, self.in_features)}，实际为{self.weight_mu.shape}\"\n",
    "            assert self.bias_mu.shape == (self.out_features,), \\\n",
    "            f\"偏置mu形状应为{(self.out_features,)}, 实际为{self.bias_mu.shape}\"\n",
    "\n",
    "            eps_weight = torch.randn_like(self.weight_mu)\n",
    "            eps_bias = torch.randn_like(self.bias_mu)\n",
    "\n",
    "            weight = self.weight_mu + weight_sigma * eps_weight\n",
    "            bias = self.bias_mu + bias_sigma * eps_bias\n",
    "\n",
    "            self.kl = self.kl_divergence(weight, weight_sigma, bias, bias_sigma)\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "\n",
    "        output = F.linear(x, weight, bias)\n",
    "\n",
    "    # 输出维度验证\n",
    "        assert output.size(-1) == self.out_features, \\\n",
    "        f\"输出特征维度{output.size(-1)}与层输出特征数{self.out_features}不匹配\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    def kl_divergence(self, weight, weight_sigma, bias, bias_sigma):\n",
    "        # 权重后验分布\n",
    "        weight_post = torch.distributions.Normal(self.weight_mu, weight_sigma)\n",
    "        # 偏置后验分布\n",
    "        bias_post = torch.distributions.Normal(self.bias_mu, bias_sigma)\n",
    "\n",
    "        # 计算权重KL散度\n",
    "        kl_weight = torch.sum(torch.distributions.kl.kl_divergence(\n",
    "            weight_post,\n",
    "            torch.distributions.Normal(0, self.prior_sigma1)\n",
    "        ))\n",
    "        # 计算偏置KL散度\n",
    "        kl_bias = torch.sum(torch.distributions.kl.kl_divergence(\n",
    "            bias_post,\n",
    "            torch.distributions.Normal(0, self.prior_sigma1)\n",
    "        ))\n",
    "        return kl_weight + kl_bias"
   ],
   "id": "bf45991fa3e4f182",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data shape verification ===\n",
      "Training data shape: (676, 69)\n",
      "Training label shape: (676,)\n",
      "Test data shape: (43, 69)\n",
      "Test label shape: (43,)\n",
      "Unique label value: [0 1 2 3]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T06:00:43.235334Z",
     "start_time": "2025-04-20T06:00:43.227260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class BayesianMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=69,\n",
    "        hidden_dims=(256, 128, 64),\n",
    "        output_dim=4,\n",
    "        dropout_prob=0.1,\n",
    "        prior_sigma1=1.0,\n",
    "        prior_sigma2=0.1,\n",
    "        prior_pi=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(\n",
    "                BayesianLinear(\n",
    "                    prev_dim,\n",
    "                    hidden_dim,\n",
    "                    prior_sigma1=prior_sigma1,\n",
    "                    prior_sigma2=prior_sigma2,\n",
    "                    prior_pi=prior_pi\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.output_layer = BayesianLinear(\n",
    "            prev_dim,\n",
    "            output_dim,\n",
    "            prior_sigma1=prior_sigma1,\n",
    "            prior_sigma2=prior_sigma2,\n",
    "            prior_pi=prior_pi\n",
    "        )\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "        self.kl = 0\n",
    "        x = self.layers(x)\n",
    "        x = self.output_layer(x, sample)\n",
    "        self.kl += self.output_layer.kl\n",
    "        return x\n",
    "\n",
    "    def get_kl(self):\n",
    "        return self.kl\n",
    "\n",
    "class SklearnBayesianMLP(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_dims=(256, 128, 64), dropout_prob=0.1,\n",
    "                 prior_sigma1=1.0, prior_sigma2=0.1, prior_pi=0.5,\n",
    "                 lr=1e-3, epochs=50, batch_size=32, device='cuda'):\n",
    "        # 增强类型转换逻辑\n",
    "        if isinstance(hidden_dims, np.ndarray):\n",
    "            self.hidden_dims = tuple(hidden_dims.astype(int).tolist())\n",
    "        elif isinstance(hidden_dims, list):\n",
    "            self.hidden_dims = tuple(hidden_dims)\n",
    "        else:\n",
    "            self.hidden_dims = hidden_dims\n",
    "\n",
    "        # 确保所有数值参数为Python原生类型\n",
    "        self.dropout_prob = float(dropout_prob)\n",
    "        self.prior_sigma1 = float(prior_sigma1)\n",
    "        self.prior_sigma2 = float(prior_sigma2)\n",
    "        self.prior_pi = float(prior_pi)\n",
    "        self.lr = float(lr)\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert data to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X).float().to(self.device)\n",
    "        y_tensor = torch.tensor(y).long().to(self.device)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = BayesianMLP(\n",
    "            input_dim=X.shape[1],\n",
    "            hidden_dims=self.hidden_dims,\n",
    "            dropout_prob=self.dropout_prob,\n",
    "            prior_sigma1=self.prior_sigma1,\n",
    "            prior_sigma2=self.prior_sigma2,\n",
    "            prior_pi=self.prior_pi\n",
    "        ).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            for inputs, targets in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets) + 1e-3 * self.model.get_kl()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X).float().to(self.device)\n",
    "            outputs = self.model(X_tensor, sample=False)\n",
    "            return torch.argmax(outputs, dim=1).cpu().numpy()"
   ],
   "id": "5279669d4d310c48",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T06:00:43.246851Z",
     "start_time": "2025-04-20T06:00:43.242831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Implementation of meta-model components\n",
    "class MetaController:\n",
    "    \"\"\"动态学习率调整的元控制器\"\"\"\n",
    "    def __init__(self, init_lr=1e-3, decay_factor=0.5, patience=3):\n",
    "        self.lr = init_lr\n",
    "        self.decay_factor = decay_factor\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "    def update(self, val_loss):\n",
    "        \"\"\"根据验证损失调整学习率\"\"\"\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.lr *= self.decay_factor\n",
    "                self.wait = 0\n",
    "                print(f\"MetaController: 学习率衰减至 {self.lr:.2e}\")"
   ],
   "id": "534d66bca4314923",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T06:00:43.281909Z",
     "start_time": "2025-04-20T06:00:43.275629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BayesianEnsemble:\n",
    "    \"\"\"BNN integration model based on uncertainty\"\"\"\n",
    "    def __init__(self, n_models=5):\n",
    "        self.n_models = n_models\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, train_features, train_labels, param_space, device='cuda'):\n",
    "        opt = BayesSearchCV(\n",
    "            estimator=SklearnBayesianMLP(device=device),  # Use wrapped model\n",
    "            search_spaces=param_space,\n",
    "            n_iter=10,\n",
    "            cv=3,\n",
    "            n_jobs=-1 if device == 'cuda' else 1\n",
    "        )\n",
    "        opt.fit(train_features, train_labels)\n",
    "\n",
    "        # Train ensemble with best params\n",
    "        for _ in range(self.n_models):\n",
    "            model = SklearnBayesianMLP(**opt.best_params_)\n",
    "            model.fit(train_features, train_labels)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X, num_samples=100):\n",
    "        \"\"\"加权集成预测（权重=1/熵）\"\"\"\n",
    "        all_probs = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            probs = []\n",
    "            for _ in range(num_samples):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(X, sample=True)\n",
    "                    prob = F.softmax(outputs, dim=1)\n",
    "                    probs.append(prob.cpu().numpy())\n",
    "            mean_probs = np.mean(probs, axis=0)\n",
    "            entropy = -np.sum(mean_probs * np.log(mean_probs + 1e-8), axis=1)\n",
    "            weights = 1 / (entropy + 1e-8)\n",
    "            all_probs.append(mean_probs * weights[:, None])\n",
    "\n",
    "        final_probs = np.mean(all_probs, axis=0)\n",
    "        return np.argmax(final_probs, axis=1)\n",
    "\n",
    "def evaluate(model, dataloader, criterion=nn.CrossEntropyLoss(), scaling_factor=1e-3, num_samples=30, device='cuda'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            # 多次采样计算平均损失\n",
    "            batch_loss = 0.0\n",
    "            for _ in range(num_samples):\n",
    "                outputs = model(inputs, sample=True)\n",
    "                loss = criterion(outputs, targets)\n",
    "                kl = model.get_kl()\n",
    "                batch_loss += (loss + scaling_factor * kl).item()\n",
    "\n",
    "            total_loss += (batch_loss / num_samples) * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    return total_loss / total_samples"
   ],
   "id": "91871a719e4fbc31",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T06:00:43.322315Z",
     "start_time": "2025-04-20T06:00:43.290914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% 5. 完整训练流程\n",
    "def main():\n",
    "    # === 1. 数据加载与设备检测 ===\n",
    "    (train_features, train_labels), (test_features, test_labels) = load_data()\n",
    "\n",
    "    # 自动检测设备并确保与Tensor设备一致\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    # === 2. 定义超参数搜索空间 ===\n",
    "    param_space = {\n",
    "        'hidden_dims': Categorical([(256, 128), (128, 64, 32), (512, 256)]),\n",
    "        'dropout_prob': Real(0.1, 0.5, prior='uniform'),\n",
    "        'prior_sigma1': Real(0.5, 2.0),\n",
    "        'prior_sigma2': Real(0.1, 0.5),\n",
    "        'lr': Real(1e-4, 1e-2, prior='log-uniform'),\n",
    "        'batch_size': Integer(16, 64)\n",
    "    }\n",
    "\n",
    "    # === 3. 训练贝叶斯集成模型 ===\n",
    "    print(\"\\n=== 开始贝叶斯优化与集成训练 ===\")\n",
    "    ensemble = BayesianEnsemble(n_models=3)\n",
    "\n",
    "    try:\n",
    "        ensemble.fit(\n",
    "            train_features=train_features,\n",
    "            train_labels=train_labels,\n",
    "            param_space=param_space,\n",
    "            device=device\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"集成训练失败: {str(e)}\")\n",
    "        return\n",
    "    # 添加参数类型验证\n",
    "    print(\"\\n=== 最佳参数类型验证 ===\")\n",
    "    sample_model = ensemble.models[0]\n",
    "    params = {\n",
    "        'hidden_dims': sample_model.hidden_dims,\n",
    "        'dropout_prob': sample_model.dropout_prob,\n",
    "        'prior_sigma1': sample_model.prior_sigma1,\n",
    "        'prior_sigma2': sample_model.prior_sigma2,\n",
    "        'lr': sample_model.lr,\n",
    "        'batch_size': sample_model.batch_size\n",
    "    }\n",
    "    for k, v in params.items():\n",
    "        print(f\"{k}: {type(v)} -> {v}\")\n",
    "    # === 4. 训练单个最佳模型（带动态学习率调整）===\n",
    "    print(\"\\n=== 训练最佳模型（带动态学习率）===\")\n",
    "\n",
    "    # 4.1 初始化模型与元控制器\n",
    "    try:\n",
    "        # 从集成模型中获取最佳参数（需确保参数类型转换）\n",
    "        best_params = ensemble.models[0].get_params()\n",
    "        best_params['device'] = device  # 确保设备参数正确传递\n",
    "\n",
    "        best_model = BayesianMLP(\n",
    "            input_dim=train_features.shape[1],\n",
    "            hidden_dims=tuple(best_params['hidden_dims']),  # 显式转换为tuple\n",
    "            dropout_prob=float(best_params['dropout_prob']),\n",
    "            prior_sigma1=float(best_params['prior_sigma1']),\n",
    "            prior_sigma2=float(best_params['prior_sigma2']),\n",
    "        ).to(device)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"参数错误: 缺少必要参数 {str(e)}\")\n",
    "        return\n",
    "\n",
    "    meta_controller = MetaController(init_lr=1e-3, decay_factor=0.5, patience=3)\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=meta_controller.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaling_factor = 1e-3  # 需与集成训练时一致\n",
    "\n",
    "    # 4.2 创建数据加载器（使用完整训练集）\n",
    "    try:\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(train_features).float().to(device),\n",
    "            torch.tensor(train_labels).long().to(device)\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"数据加载失败: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 4.3 划分验证集（10%训练集）\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    train_idx, val_idx = next(kf.split(train_features))\n",
    "\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(train_features[val_idx]).float().to(device),\n",
    "        torch.tensor(train_labels[val_idx]).long().to(device)\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    # 4.4 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(100):\n",
    "        # 训练步骤\n",
    "        best_model.train()\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(inputs)\n",
    "            loss = criterion(outputs, targets) + scaling_factor * best_model.get_kl()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "        # 验证步骤\n",
    "        val_loss = evaluate(\n",
    "            model=best_model,\n",
    "            dataloader=val_loader,\n",
    "            criterion=criterion,\n",
    "            scaling_factor=scaling_factor,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # 动态学习率调整\n",
    "        meta_controller.update(val_loss)\n",
    "        optimizer.param_groups[0]['lr'] = meta_controller.lr\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(best_model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"LR: {meta_controller.lr:.2e}\")\n",
    "\n",
    "    # === 5. 最终评估 ===\n",
    "    print(\"\\n=== 最终评估 ===\")\n",
    "\n",
    "    # 5.1 加载最佳模型\n",
    "    try:\n",
    "        best_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    except FileNotFoundError:\n",
    "        print(\"找不到模型文件\")\n",
    "        return\n",
    "\n",
    "    # 5.2 评估单一模型\n",
    "    test_tensor = torch.tensor(test_features).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(test_tensor, sample=False)\n",
    "    single_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    print(f\"单一模型测试准确率: {accuracy_score(test_labels, single_preds):.4f}\")\n",
    "    print(f\"单一模型F1分数: {f1_score(test_labels, single_preds, average='macro'):.4f}\")\n",
    "\n",
    "    # 5.3 评估集成模型\n",
    "    try:\n",
    "        ensemble_preds = ensemble.predict(test_tensor)\n",
    "        print(f\"\\n集成模型测试准确率: {accuracy_score(test_labels, ensemble_preds):.4f}\")\n",
    "        print(f\"集成模型F1分数: {f1_score(test_labels, ensemble_preds, average='macro'):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"集成模型评估失败: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "869f46914d0037d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data shape verification ===\n",
      "Training data shape: (676, 69)\n",
      "Training label shape: (676,)\n",
      "Test data shape: (43, 69)\n",
      "Test label shape: (43,)\n",
      "Unique label value: [0 1 2 3]\n",
      "使用设备: cpu\n",
      "\n",
      "=== 开始贝叶斯优化与集成训练 ===\n",
      "集成训练失败: can only convert an array of size 1 to a Python scalar\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T06:00:43.330749Z",
     "start_time": "2025-04-20T06:00:43.328745Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T06:00:43.377224Z",
     "start_time": "2025-04-20T06:00:43.375234Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "95bd348ff4c2914",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
