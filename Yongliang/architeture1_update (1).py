# -*- coding: utf-8 -*-
"""architeture1_update.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10yVgsK3oRp3hqY1NaC1-2IQJUL430PD7
"""

#!/usr/bin/env python3
"""
Ensemble Learning Code: Base models (Logistic, KNN, SVM, RF, XGBoost, LightGBM, ExtraTrees, NB),
hyperparameter tuning via GridSearchCV, calibration, stacking, and evaluation.
"""

import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import accuracy_score, precision_score, f1_score


# Set random seed for reproducibility
seed = 42
np.random.seed(seed)
random.seed(seed)

# Try to import LightGBM; skip if not available
try:
    from lightgbm import LGBMClassifier
    lightgbm_available = True
except ImportError:
    lightgbm_available = False
    print("Warning: lightgbm is not installed. LightGBM model will be skipped.")

def load_data(train_path, test_path):
    """
    Load training and test data from CSV files.
    Assumes target column is named 'failure mode'.
    """
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    # Separate features and target
    X_train = train_df.drop('failure mode', axis=1).values
    y_train = train_df['failure mode'].values
    X_test = test_df.drop('failure mode', axis=1).values
    y_test = test_df['failure mode'].values
    return X_train, y_train, X_test, y_test

def define_models_and_grids():
    """
    Define base models and their hyperparameter grids.
    Returns:
        models: dict of model name to scikit-learn Pipeline or estimator.
        param_grids: dict of model name to hyperparameter grid.
    """
    models = {}
    param_grids = {}

    # Logistic Regression (with StandardScaler)
    lr_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', LogisticRegression(random_state=seed, max_iter=1000))
    ])
    models['Logistic Regression'] = lr_pipeline
    param_grids['Logistic Regression'] = {
        'clf__C': [0.1, 1, 10],
        'clf__penalty': ['l2'],
        'clf__solver': ['lbfgs'],
        'clf__multi_class': ['auto']
    }

    # K-Nearest Neighbors (with StandardScaler)
    knn_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', KNeighborsClassifier())
    ])
    models['K-Nearest Neighbors'] = knn_pipeline
    param_grids['K-Nearest Neighbors'] = {
        'clf__n_neighbors': [3, 5, 7],
        'clf__weights': ['uniform', 'distance']
    }

    # Support Vector Machine (RBF kernel, with StandardScaler)
    svm_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', SVC(kernel='rbf', probability=True, random_state=seed))
    ])
    models['SVM (RBF)'] = svm_pipeline
    param_grids['SVM (RBF)'] = {
        'clf__C': [0.1, 1, 10],
        'clf__gamma': ['scale', 'auto']
    }

    # Random Forest (no scaling needed)
    rf_pipeline = Pipeline([
        ('clf', RandomForestClassifier(random_state=seed))
    ])
    models['Random Forest'] = rf_pipeline
    param_grids['Random Forest'] = {
        'clf__n_estimators': [100, 200,300],
        'clf__max_depth': [None, 10, 20,30],
        'clf__min_samples_split': [2, 4,5]
    }

    # XGBoost (no scaling needed)
    xgb_pipeline = Pipeline([
        ('clf', XGBClassifier(random_state=seed, use_label_encoder=False, eval_metric='mlogloss'))
    ])
    models['XGBoost'] = xgb_pipeline
    param_grids['XGBoost'] = {
        'clf__n_estimators': [100, 200],
        'clf__max_depth': [None, 10, 20],
        'clf__learning_rate': [0.01, 0.1],
        'clf__subsample': [0.6,0.8, 1.0]
    }

    # LightGBM (no scaling needed), if available
    if lightgbm_available:
        lgbm_pipeline = Pipeline([
            ('clf', LGBMClassifier(random_state=seed))
        ])
        models['LightGBM'] = lgbm_pipeline
        param_grids['LightGBM'] = {
            'clf__n_estimators': [100, 200],
            'clf__num_leaves': [31, 50],
            'clf__learning_rate': [0.01, 0.1]
        }

    # Extra Trees (no scaling needed)
    et_pipeline = Pipeline([
        ('clf', ExtraTreesClassifier(random_state=seed))
    ])
    models['Extra Trees'] = et_pipeline
    param_grids['Extra Trees'] = {
        'clf__n_estimators': [100, 200,300],
        'clf__max_depth': [None, 10, 20],
        'clf__min_samples_split': [2, 5]
    }

    # Gaussian Naive Bayes (no scaling needed)
    nb_pipeline = Pipeline([
        ('clf', GaussianNB())
    ])
    models['Gaussian NB'] = nb_pipeline
    param_grids['Gaussian NB'] = {
        'clf__var_smoothing': [1e-9, 1e-8, 1e-7]
    }

    return models, param_grids

def tune_models(X, y, models, param_grids, cv):
    """
    Perform GridSearchCV to tune hyperparameters for each model.
    Returns a dictionary of best-estimator models.
    """
    best_models = {}
    for name, model in models.items():
        print(f"Tuning {name}...")
        grid = GridSearchCV(model, param_grids[name], cv=cv, n_jobs=-1)
        grid.fit(X, y)
        best_models[name] = grid.best_estimator_
        print(f"Best params for {name}: {grid.best_params_}")
    return best_models

def calibrate_models(best_models, X, y, cv):
    """
    Calibrate each tuned model using CalibratedClassifierCV (Platt scaling).
    Returns a dictionary of calibrated classifier models.
    """
    calibrated_models = {}
    for name, model in best_models.items():
        print(f"Calibrating {name}...")
        # Use cv on calibration to avoid data leakage; clones base estimator internally
        calibrator = CalibratedClassifierCV(estimator=model, method='isotonic',cv=cv)
        #calibrator = CalibratedClassifierCV(estimator=model, method='sigmoid', cv=cv)
        calibrator.fit(X, y)
        calibrated_models[name] = calibrator
    return calibrated_models

def generate_oof_predictions(best_models, X, y, cv):
    """
#    Generate out-of-fold (OOF) predictions (probabilities) for each base model.
#    Returns a 2D array of shape (n_samples, n_models * n_classes).
#    """
    oof_list = []
    for name, model in best_models.items():
        print(f"Generating OOF predictions for {name}...")
        probs = cross_val_predict(model, X, y, cv=cv, method='predict_proba', n_jobs=-1)
        oof_list.append(probs)
     #Concatenate horizontally to form meta-features
     #Each element in oof_list is (n_samples, n_classes)
    X_meta = np.hstack(oof_list)
    return X_meta

def train_meta_learner(X_meta, y):
    """
    Train a Logistic Regression meta-learner on OOF predictions.
    """
    print("Training meta-learner (Logistic Regression)...")
    meta = LogisticRegression(random_state=seed, max_iter=1000)
    meta.fit(X_meta, y)
    return meta

def evaluate_models(models, X_test, y_test):
    """
    Evaluate given models on test data.
    Returns a list of tuples: (model_name, accuracy, macro_precision, macro_f1).
    """
    results = []
    for name, model in models.items():
        preds = model.predict(X_test)
        acc = accuracy_score(y_test, preds)
        prec = precision_score(y_test, preds, average='macro', zero_division=0)
        f1 = f1_score(y_test, preds, average='macro', zero_division=0)
        results.append((name, acc, prec, f1))
    return results

def main():
    # Load data
    X_train, y_train, X_test, y_test = load_data('train_smote.csv', 'test_original.csv')


    from sklearn.preprocessing import LabelEncoder

    le = LabelEncoder()
    y_train = le.fit_transform(y_train)
    y_test = le.fit_transform(y_test)


    # Stratified CV setup
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)

    # Define models and hyperparameter grids
    models, param_grids = define_models_and_grids()

    # Hyperparameter tuning
    best_models = tune_models(X_train, y_train, models, param_grids, cv)

    # Calibrate tuned models
    calibrated_models = calibrate_models(best_models, X_train, y_train, cv)

    # Generate OOF predictions for stacking
    X_meta_train = generate_oof_predictions(calibrated_models, X_train, y_train, cv)

    # Train meta-learner
    meta_model = train_meta_learner(X_meta_train, y_train)

    # Evaluate base models on test data
    base_results = []
    for name in best_models:
        # Use calibrated model for final predictions
        calibrator = calibrated_models[name]
        y_pred = calibrator.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, average='macro', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
        base_results.append((name, acc, prec, f1))

    # Stacked ensemble (meta-learner)
    print("Generating Stacked ensemble predictions...")
    meta_features_test = []
    for name in calibrated_models:
        probs = calibrated_models[name].predict_proba(X_test)
        meta_features_test.append(probs)
    X_meta_test = np.hstack(meta_features_test)
    stacked_preds = meta_model.predict(X_meta_test)
    acc = accuracy_score(y_test, stacked_preds)
    prec = precision_score(y_test, stacked_preds, average='macro', zero_division=0)
    f1 = f1_score(y_test, stacked_preds, average='macro', zero_division=0)
    base_results.append(("Stacked Ensemble", acc, prec, f1))

    # Present results in a clean table
    results_df = pd.DataFrame(base_results, columns=['Model', 'Accuracy', 'Macro Precision', 'Macro F1'])
    print("\nFinal Evaluation Results:")
    print(results_df.to_string(index=False, float_format='%.4f'))

    from sklearn.metrics import classification_report, confusion_matrix

    stacked_preds = meta_model.predict(X_meta_test)  # or re-generate from base models
    print("Stacked Ensemble Classification Report:")
    print(classification_report(y_test, stacked_preds, digits=4))

    cm = confusion_matrix(y_test, stacked_preds)

    class_acc = cm.diagonal() / cm.sum(axis=1)
    for i, acc in enumerate(class_acc):
        print(f"Class {i} accuracy: {acc:.4f}")
    #1
    acc = accuracy_score(y_test, stacked_preds)

    #   Macro‐F1 (averages per‐class F1 equally, good for multi‐class balanced/unbalanced):
    f1_macro = f1_score(y_test, stacked_preds, average='macro')

    print(f"Stacked Ensemble Accuracy: {acc:.4f}")
    print(f"Stacked Ensemble F1 (macro): {f1_macro:.4f}\n")


    #plot
    # 4.2. Define your class‐names / tick‐labels.
    #      If your labels are integers {1,2,3,4}, you can do:
    class_labels = ["1", "2", "3", "4"]

    #      If they are zero‐based (0,1,2,3), adjust accordingly:
    #      class_labels = ["0","1","2","3"]

    # 4.3. Plot with seaborn
    plt.figure(figsize=(6,5))
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        cbar=True,
        xticklabels=class_labels,
        yticklabels=class_labels
    )
    plt.xlabel("Predicted Label", fontsize=12)
    plt.ylabel("True Label", fontsize=12)
    plt.title("Confusion Matrix – Stacked Ensemble", fontsize=14)
    plt.tight_layout()
    plt.show()
if __name__ == "__main__":
    main()

# 1. import libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    confusion_matrix,
    classification_report
)

# 3. Compute Accuracy and F1‐Score

#   Accuracy (overall fraction correct):
acc = accuracy_score(y_test, stacked_preds)

#   Macro‐F1 (averages per‐class F1 equally, good for multi‐class balanced/unbalanced):
f1_macro = f1_score(y_test, stacked_preds, average='macro')

print(f"Stacked Ensemble Accuracy: {acc:.4f}")
print(f"Stacked Ensemble F1 (macro): {f1_macro:.4f}\n")

#   (Optionally print a full classification report:)
print("Classification Report:\n")
print(classification_report(y_test, stacked_preds, digits=4))

# 4. Build and plot the Confusion Matrix

# 4.1. Compute the confusion matrix (rows = true labels, columns = predicted labels)
cm = confusion_matrix(y_test, stacked_preds)

# 4.2. Define your class‐names / tick‐labels.
#      If your labels are integers {1,2,3,4}, you can do:
class_labels = ["1", "2", "3", "4"]

#      If they are zero‐based (0,1,2,3), adjust accordingly:
#      class_labels = ["0","1","2","3"]

# 4.3. Plot with seaborn
plt.figure(figsize=(6,5))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    cbar=True,
    xticklabels=class_labels,
    yticklabels=class_labels
)
plt.xlabel("Predicted Label", fontsize=12)
plt.ylabel("True Label", fontsize=12)
plt.title("Confusion Matrix – Stacked Ensemble", fontsize=14)
plt.tight_layout()
plt.show()

Stacked Ensemble Classification Report:
              precision    recall  f1-score   support

           0     0.8810    1.0000    0.9367        37
           1     0.7500    0.5000    0.6000         6
           2     0.9722    0.9211    0.9459        38
           3     1.0000    0.8000    0.8889         5

    accuracy                         0.9186        86
   macro avg     0.9008    0.8053    0.8429        86
weighted avg     0.9191    0.9186    0.9145        86

Class 0 accuracy: 1.0000
Class 1 accuracy: 0.5000
Class 2 accuracy: 0.9211
Class 3 accuracy: 0.8000